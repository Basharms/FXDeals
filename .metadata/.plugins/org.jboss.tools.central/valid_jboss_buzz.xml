<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Bend-points and the DMN Editor</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Oixfk8_svDY/bend-points-and-the-dmn-editor.html" /><author><name>Daniel José dos Santos</name></author><id>https://blog.kie.org/2021/07/bend-points-and-the-dmn-editor.html</id><updated>2021-07-30T13:44:26Z</updated><content type="html">We’ve been working in DMN Editor to improve its user experience. We know that our users sometimes have models with many edges that overlap nodes, other edges and are hard to arrange. Even with features like the that help users on organizing the content, there are still some cases where users cannot or don’t want to split the diagram. But how can they deal with this? Users may connect the "NumC" and the "Basic" nodes, but is this the best way to organize the nodes? A simple solution is already available in the BPMN editor, and now implemented in the DMN editor relies on the use of bend-points for this kind of scenario. This feature enables users to create "flexible edges". It may be easier to reshape the edge by going around the other edges with this feature instead of rearranging the nodes, so that the edges do not overlap. PREVENTING THE PAIN The BPMN and the DMN editors rely on the same core diagram system, each of them with specific features for each use case. The beautiful thing is that when the BPMN introduced bend-points, we had in mind that it would be supported for DMN at some point too, so it was built in a way that it was decoupled from the BPMN editor, envisioning re-usability in other places than BPMN. So, on DMN, we have an Edge with a list of waypoints but only filled with two points: the source and the target node. Newcomers may think that developers from the past were doing bad coding using a list for keeping only two points, but it was made this way, keeping in mind the bend-points. So, when we finally enabled the bend-points, the DMN editor structure was ready to handle Edges of lists of waypoints. I like to point to this case and show how we, as developers, may keep our code ready for expansions, especially if it is a new feature that we already have on our radar like this one. It saves time for people from the future who maybe are ourselves. The support for bend-points will be available in the next Kogito release. Stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Oixfk8_svDY" height="1" width="1" alt=""/&gt;</content><dc:creator>Daniel José dos Santos</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/bend-points-and-the-dmn-editor.html</feedburner:origLink></entry><entry><title>Avoiding dual writes in event-driven applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aN8toxH78qM/avoiding-dual-writes-event-driven-applications" /><author><name>Bernard Tison</name></author><id>41c4e1a8-fdd0-49fb-9b56-053be5d91b86</id><updated>2021-07-30T07:00:00Z</updated><published>2021-07-30T07:00:00Z</published><summary type="html">&lt;p&gt;Dual writes frequently cause issues in distributed, event-driven applications. A &lt;em&gt;dual write&lt;/em&gt; occurs when an application has to change data in two different systems, such as when an application needs to persist data in the database and send a Kafka message to notify other systems. If one of these two operations fails, you might end up with inconsistent data. Dual writes can be hard to detect and fix.&lt;/p&gt; &lt;p&gt;In this article, you will learn how to use the outbox pattern with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; to avoid the dual write problem in event-driven applications. I will show you how to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Provision a Kafka cluster on OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Deploy and configure Debezium to use OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Run an application that uses Debezium and OpenShift Streams for Apache Kafka to implement the outbox pattern.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;OpenShift Streams for Apache Kafka&lt;/a&gt; is an Apache Kafka service that is fully hosted and managed by Red Hat. The service is useful for developers who want to incorporate streaming data and scalable messaging in their applications without the burden of setting up and maintaining a Kafka cluster infrastructure.&lt;/p&gt; &lt;p&gt;&lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; is an open source, distributed platform for change data capture. Built on top of Apache Kafka, Debezium allows applications to react to inserts, updates, and deletes in your databases.&lt;/p&gt; &lt;h2&gt;Demo: Dual writes in an event-driven system&lt;/h2&gt; &lt;p&gt;The demo application we'll use in this article is part of a distributed, event-driven order management system. The application suffers from the dual write issue: When a new order comes in through a REST interface, the order is persisted in the database—in this case PostgreSQL—and an &lt;code&gt;OrderCreated&lt;/code&gt; event is sent to a Kafka topic. From there, it can be consumed by other parts of the system.&lt;/p&gt; &lt;p&gt;You will find the code for the order service application in this &lt;a href="https://github.com/rhosak-debezium-outbox/order-service"&gt;Github repository&lt;/a&gt;. The application was developed using &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;. The structure of the &lt;code&gt;OrderCreated&lt;/code&gt; events follows the &lt;a href="https://cloudevents.io"&gt;CloudEvents&lt;/a&gt; specification, which defines a common way to describe event data.&lt;/p&gt; &lt;h3&gt;Solving dual writes with the outbox pattern&lt;/h3&gt; &lt;p&gt;Figure 1 shows the architecture of the outbox pattern implemented with Debezium and OpenShift Streams for Apache Kafka. For this example, you will also use Docker to spin up the different application components on your local system.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/arch.png?itok=LtcRgFRW" width="600" height="338" alt="In the outbox pattern with Apache Kafka, Debezium monitors inserts and informs Kafka, which in turn tells the event consumers about the change." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Architecture of the outbox pattern with Debezium and OpenShift Streams for Apache Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: For a thorough discussion of Debezium and the outbox pattern see &lt;a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/"&gt;Reliable Microservices Data Exchange With the Outbox Pattern&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Prerequisites for the demonstration&lt;/h3&gt; &lt;p&gt;This article assumes that you already have an OpenShift Streams for Apache Kafka instance in your development environment. Visit the &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; page to create a Kafka instance. See the article &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; for the basics of creating Kafka instances, topics, and service accounts.&lt;/p&gt; &lt;p&gt;I also assume that you have Docker installed on your local system.&lt;/p&gt; &lt;h2&gt;Provision a Kafka cluster with OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;After you set up an OpenShift Streams for Apache Kafka instance, you will create environment variables for the Kafka bootstrap server endpoint and the service account credentials. Use the following environment variables when configuring the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export KAFKA_BOOTSTRAP_SERVER=&lt;value of the Bootstrap server endpoint&gt; $ export CLIENT_ID=&lt;value of the service account Client ID&gt; $ export CLIENT_SECRET=&lt;value of the service account Client Secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a topic named &lt;code&gt;order-event&lt;/code&gt; on your Kafka instance for the user service application, as shown in Figure 2. The number of partitions is not critical for this example. I generally use 15 partitions for Kafka topics as a default. You can leave the message retention time at seven days.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topic.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/topic.png?itok=5CDjuuoC" width="600" height="392" alt="The Kafka instance for the outbox pattern shows information about the order-event." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The order-event Kafka topic on OpenShift Streams for Apache Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Persisting the order service&lt;/h3&gt; &lt;p&gt;The order service application exposes a REST endpoint for new orders. When a new order is received, the order is persisted using JPA in the &lt;code&gt;orders&lt;/code&gt; table of the PostgreSQL database. In the same transaction, the outbox event for the &lt;code&gt;OrderCreated&lt;/code&gt; message is written to the &lt;code&gt;orders_outbox&lt;/code&gt; table. See &lt;a href="https://github.com/rhosak-debezium-outbox/order-service"&gt;this Github repo&lt;/a&gt; for the application source code. The &lt;code&gt;OrderService&lt;/code&gt; class contains the code for persisting the order entity and the &lt;code&gt;OrderCreated&lt;/code&gt; message:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@ApplicationScoped public class OrderService { @Inject EntityManager entityManager; @ConfigProperty(name = "order-event.aggregate.type", defaultValue = "order-event") String aggregateType; @Transactional public Long create(Order order) { order.setStatus(OrderStatus.CREATED); entityManager.persist(order); OutboxEvent outboxEvent = buildOutBoxEvent(order); entityManager.persist(outboxEvent); entityManager.remove(outboxEvent); return order.getId(); } OutboxEvent buildOutBoxEvent(Order order) { OutboxEvent outboxEvent = new OutboxEvent(); outboxEvent.setAggregateType(aggregateType); outboxEvent.setAggregateId(Long.toString(order.getId())); outboxEvent.setContentType("application/cloudevents+json; charset=UTF-8"); outboxEvent.setPayload(toCloudEvent(order)); return outboxEvent; } String toCloudEvent(Order order) { CloudEvent event = CloudEventBuilder.v1().withType("OrderCreatedEvent") .withTime(OffsetDateTime.now()) .withSource(URI.create("ecommerce/order-service")) .withDataContentType("application/json") .withId(UUID.randomUUID().toString()) .withData(order.toJson().encode().getBytes()) .build(); EventFormat format = EventFormatProvider.getInstance() .resolveFormat(JsonFormat.CONTENT_TYPE); return new String(format.serialize(event)); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Notice the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The structure of the &lt;code&gt;OrderCreated&lt;/code&gt; message follows the CloudEvents specification. The code uses the Java SDK for CloudEvents API to build the CloudEvent and serialize it to JSON format.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;ContentType&lt;/code&gt; field of the &lt;code&gt;OutboxEvent&lt;/code&gt; entity is set to &lt;code&gt;application/cloudevents+json&lt;/code&gt;. When processed by a Debezium single message transformation (SMT), this value is set as the &lt;code&gt;content-type&lt;/code&gt; header on the Kafka message, as mandated by the CloudEvents specification.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;I'll discuss the structure of the Outbox Event table shortly. The &lt;code&gt;OutboxEvent&lt;/code&gt; entity is persisted to the database and then removed right away. Debezium, being log-based, does not examine the contents of the database table; it just tails the append-only transaction log. The code will generate an &lt;code&gt;INSERT&lt;/code&gt; and a &lt;code&gt;DELETE&lt;/code&gt; entry in the log when the transaction commits. Debezium processes both events, and produces a Kafka message for any &lt;code&gt;INSERT&lt;/code&gt;. However, &lt;code&gt;DELETE&lt;/code&gt; events are ignored.&lt;/p&gt; &lt;p&gt;The net result is that Debezium is able to capture the event added to the outbox table, but the table itself remains empty. No additional disk space is needed for the table and no separate housekeeping process is required to stop it from growing indefinitely.&lt;/p&gt; &lt;h3&gt;Running the PostgreSQL database&lt;/h3&gt; &lt;p&gt;Run the PostgreSQL database as a Docker container. The database and the &lt;code&gt;orders&lt;/code&gt; and &lt;code&gt;orders_outbox&lt;/code&gt; tables are created when the container starts up:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -d --name postgresql \ -e POSTGRESQL_USER=orders -e POSTGRESQL_PASSWORD=orders \ -e POSTGRESQL_ADMIN_PASSWORD=admin -e POSTGRESQL_DATABASE=orders \ quay.io/btison_rhosak/postgresql-order-service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, spin up the container for the order service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -d --name order-service -p 8080:8080 \ --link postgresql -e DATABASE_USER=orders \ -e DATABASE_PASSWORD=orders -e DATABASE_NAME=orders \ -e DATABASE_HOST=postgresql -e ORDER_EVENT_AGGREGATE_TYPE=order-event \ quay.io/btison_rhosak/order-service-outbox&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configure and run Debezium&lt;/h2&gt; &lt;p&gt;Debezium is implemented as a &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Kafka Connect&lt;/a&gt; connector, so the first thing to do is to spin up a Kafka Connect container, pointing to the managed Kafka instance.&lt;/p&gt; &lt;p&gt;The Kafka Connect image we use here is derived from the Kafka Connect image provided by the &lt;a href="https://strimzi.io"&gt;Strimzi&lt;/a&gt; project. The Debezium libraries and the Debezium PostgreSQL connector are already installed on this image.&lt;/p&gt; &lt;p&gt;Kafka Connect is configured using a properties file, which specifies among other things how Kafka Connect should connect to the Kafka broker.&lt;/p&gt; &lt;p&gt;To connect to the managed Kafka instance, this application employs &lt;a href="https://docs.confluent.io/platform/current/kafka/authentication_sasl/authentication_sasl_plain.html"&gt;SASL/PLAIN&lt;/a&gt; authentication over TLS, using the client ID and secret from the service account you created earlier as credentials.&lt;/p&gt; &lt;h3&gt;Creating a configuration file&lt;/h3&gt; &lt;p&gt;Create a configuration file on your local file system. The file refers to environment variables for the Kafka bootstrap address and the service account credentials:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ cat &lt;&lt; EOF &gt; /tmp/kafka-connect.properties # Bootstrap servers bootstrap.servers=$KAFKA_BOOTSTRAP_SERVER # REST Listeners rest.port=8083 # Plugins plugin.path=/opt/kafka/plugins # Provided configuration offset.storage.topic=kafka-connect-offsets value.converter=org.apache.kafka.connect.json.JsonConverter config.storage.topic=kafka-connect-configs key.converter=org.apache.kafka.connect.json.JsonConverter group.id=kafka-connect status.storage.topic=kafka-connect-status config.storage.replication.factor=3 key.converter.schemas.enable=false offset.storage.replication.factor=3 status.storage.replication.factor=3 value.converter.schemas.enable=false security.protocol=SASL_SSL producer.security.protocol=SASL_SSL consumer.security.protocol=SASL_SSL admin.security.protocol=SASL_SSL sasl.mechanism=PLAIN producer.sasl.mechanism=PLAIN consumer.sasl.mechanism=PLAIN admin.sasl.mechanism=PLAIN sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; producer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; admin.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \ username="$CLIENT_ID" \ password="$CLIENT_SECRET" ; EOF&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Launching the Kafka Connect container&lt;/h3&gt; &lt;p&gt;Next, you will launch the Kafka Connect container. The properties file you just created is mounted into the container. The container is also linked to the PostgreSQL container, so that the Debezium connector can connect to PostgreSQL to access the transaction logs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -d --name kafka-connect --link postgresql -p 8083:8083 \ --mount type=bind,source=/tmp/kafka-connect.properties,destination=/config/kafka-connect.properties \ quay.io/btison_rhosak/kafka-connect-dbz-pgsql:1.7.0-1.5.0.Final&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now check the Kafka Connect container logs. Kafka Connect logs are quite verbose, so if you don’t see any stack traces, you can assume Kafka Connect is running fine and successfully connected to the Kafka cluster.&lt;/p&gt; &lt;h3&gt;Configuring the Debezium connector&lt;/h3&gt; &lt;p&gt;Kafka Connect exposes a REST endpoint through which you can deploy and manage Kafka Connect connectors, such as the Debezium connector. Create a file on your local file system for the Debezium connector configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ cat &lt;&lt; EOF &gt; /tmp/debezium-connector.json { "name": "debezium-postgres-orders", "config": { "connector.class": "io.debezium.connector.postgresql.PostgresConnector", "plugin.name": "pgoutput", "database.hostname": "postgresql", "database.port": "5432", "database.user": "postgres", "database.password": "admin", "database.dbname": "orders", "database.server.name": "orders1", "schema.whitelist": "public", "table.whitelist": "public.orders_outbox", "tombstones.on.delete" : "false", "transforms": "router", "transforms.router.type": "io.debezium.transforms.outbox.EventRouter", "transforms.router.table.fields.additional.placement": "content_type:header:content-type", "transforms.router.route.topic.replacement": "\${routedByValue}" } } EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's look more closely at some of the fields in this configuration:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;plugin-name&lt;/strong&gt;: The Debezium connector uses a PostgreSQL output plug-in to extract changes committed to the transaction log. In this case, we use the &lt;code&gt;pgoutput&lt;/code&gt; plug-in, which is included in PostgreSQL since version 10. See the &lt;a href="https://debezium.io/documentation/reference/1.5/connectors/postgresql.html"&gt;Debezium documentation&lt;/a&gt; for information about output plug-ins.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;database.*&lt;/strong&gt;: These settings allow Debezium to connect to the PostgreSQL database. This example specifies the &lt;code&gt;postgres&lt;/code&gt; system user, which has superuser privileges. In a production system, you should probably create a dedicated Debezium user with the necessary privileges.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;table.whitelist&lt;/strong&gt;: This specifies the list of tables that are monitored for changes by the Debezium Connector.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;tombstones.on.delete&lt;/strong&gt;: This indicates whether a deletion marker ("tombstones") should be emitted by the connector when a record is deleted from the outbox table. By setting &lt;code&gt;tombstones.on.delete&lt;/code&gt; to false, you tell Debezium to effectively ignore deletes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;transforms.*&lt;/strong&gt;: These settings describe how Debezium should process database change events. Debezium applies a single message transform (SMT) to every captured change event. For the outbox pattern, Debezium uses the built-in &lt;code&gt;EventRouter&lt;/code&gt; SMT, which extracts the new state of the change event, transforms it into a Kafka message, and sends it to the appropriate topic.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;More about the EventRouter&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;EventRouter&lt;/code&gt; by default makes certain assumptions about the structure of the outbox table:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; Column | Type | Modifiers --------------+------------------------+----------- id | uuid | not null aggregatetype | character varying(255) | not null aggregateid | character varying(255) | not null payload | text | not null content_type | character varying(255) | not null &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;EventRouter&lt;/code&gt; calculates the value of the destination topic from the &lt;code&gt;aggregatetype&lt;/code&gt; column and the value of the &lt;code&gt;route.topic.replacement&lt;/code&gt; configuration (where &lt;code&gt;${routedBy}&lt;/code&gt; represents the value in the &lt;code&gt;aggregatetype&lt;/code&gt; column). The key of the Kafka message is the value of the &lt;code&gt;aggregateid&lt;/code&gt; column, and the payload is whatever is in the &lt;code&gt;payload&lt;/code&gt; column. The &lt;code&gt;table.fields.additional.placement&lt;/code&gt; parameter defines how additional columns should be handled. In our case, we specify that the value of the &lt;code&gt;content_type&lt;/code&gt; column should be added to the Kafka message as a header with key &lt;code&gt;content-type&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Deploying the Debezium connector&lt;/h3&gt; &lt;p&gt;Deploy the Debezium connector by calling the Kafka Connect REST endpoint:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X POST -H "Accept: application/json" -H "Content-type: application/json" \ -d @/tmp/debezium-connector.json 'http://localhost:8083/connectors'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the logs of the &lt;code&gt;kafka-connect&lt;/code&gt; container to verify that the Debezium connector was installed successfully. If you were successful, you’ll see something like the following toward the end of the logs:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;2021-06-09 21:09:46,944 INFO user 'postgres' connected to database 'orders' on PostgreSQL 12.5 on x86_64-redhat-linux-gnu, compiled by gcc (GCC) 8.3.1 20191121 (Red Hat 8.3.1-5), 64-bit with roles: role 'pg_read_all_settings' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_stat_scan_tables' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_write_server_files' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_monitor' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'pg_read_server_files' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: false] role 'orders' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can log in: true] role 'pg_execute_server_program' [superuser: false, replication: false, inherit: true, create role: false, create db: false,can log in: false] role 'pg_read_all_stats' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can login: false] role 'pg_signal_backend' [superuser: false, replication: false, inherit: true, create role: false, create db: false, can login: false] role 'postgres' [superuser: true, replication: true, inherit: true, create role: true, create db: true, can log in: true] (io.debezium.connector.postgresql.PostgresConnectorTask) [task-thread-debezium-postgres-orders-0] &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Consume the Kafka messages&lt;/h2&gt; &lt;p&gt;Next, you want to view the Kafka messages produced by Debezium and sent to the &lt;code&gt;order-event&lt;/code&gt; topic. You can use a tool such as &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt; to process the messages. The following &lt;code&gt;docker&lt;/code&gt; command launches a container hosting the &lt;code&gt;kafkacat&lt;/code&gt; utility and consumes all the messages in the &lt;code&gt;order-event&lt;/code&gt; topic from the beginning:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ docker run -it --rm edenhill/kafkacat:1.6.0 kafkacat \ -b $KAFKA_BOOTSTRAP_SERVER -t order-event \ -X security.protocol=SASL_SSL -X sasl.mechanisms=PLAIN \ -X sasl.username="$CLIENT_ID" -X sasl.password="$CLIENT_SECRET" \ -f 'Partition: %p\n Key: %k\n Headers: %h\n Payload: %s\n' -C&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Test the application&lt;/h2&gt; &lt;p&gt;All components are in place to test the order service application. To use the REST interface to create an order, issue the following cURL command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -v -X POST -H "Content-type: application/json" \ -d '{"customerId": "customer123", "productCode": "XXX-YYY", "quantity": 3, "price": 159.99}' \ http://localhost:8080/order&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Verify with &lt;code&gt;kafkacat&lt;/code&gt; that a Kafka message has been produced to the &lt;code&gt;order-event&lt;/code&gt; topic. When you issue the &lt;code&gt;kafkacat&lt;/code&gt; command mentioned earlier, the output should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Partition: 3 Key: "992" Headers: id=743e3736-f9e3-4c2f-bce7-eaa35afe8876,content-type=application/cloudevents+json; charset=UTF-8 Payload: "{\"specversion\":\"1.0\",\"id\":\"843d8770-f23d-41e2-a697-a64367f1d387\",\"source\":\"ecommerce/order-service\",\"type\":\"OrderCreatedEvent\",\"datacontenttype\":\"application/json\",\"time\":\"2021-06-10T07:40:52.282602Z\",\"data\":{\"id\":992,\"customerId\":\"customer123\",\"productCode\":\"XXX-YYY\",\"quantity\":3,\"price\":159.99,\"status\":\"CREATED\"}}" % Reached end of topic order-event [3] at offset 1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the ID of the outbox event is added as a header to the message. This information can be exploited by consumers for duplicate detection. The &lt;code&gt;content-type&lt;/code&gt; header is added by the Debezium &lt;code&gt;EventRouter&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Great job! If you’ve followed along, you have successfully:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Provisioned a &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;managed Kafka&lt;/a&gt; instance on &lt;a href="https://cloud.redhat.com"&gt;cloud.redhat.com&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Used &lt;a href="https://kafka.apache.org/documentation/#connect"&gt;Kafka Connect&lt;/a&gt; and &lt;a href="https://debezium.io"&gt;Debezium&lt;/a&gt; to connect to the managed Kafka instance.&lt;/li&gt; &lt;li&gt;Implemented and tested the outbox pattern with Debezium.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Setting up and maintaining a Kafka cluster can be tedious and complex. OpenShift Streams for Apache Kafka takes away that burden, allowing you to focus on implementing services and business logic.&lt;/p&gt; &lt;p&gt;Applications can connect to the managed Kafka instance from everywhere, so it doesn’t really matter where these applications run, whether it's on a private or public cloud, or even in Docker containers on your local workstation.&lt;/p&gt; &lt;p&gt;Stay tuned for more articles on interesting use cases and demos with OpenShift Streams for Apache Kafka.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications" title="Avoiding dual writes in event-driven applications"&gt;Avoiding dual writes in event-driven applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aN8toxH78qM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Bernard Tison</dc:creator><dc:date>2021-07-30T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/30/avoiding-dual-writes-event-driven-applications</feedburner:origLink></entry><entry><title type="html">Why some prometheus alerts in k8s can confuse</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FDn2otUDpuU/" /><author><name /></author><id>https://blog.ramon-gordillo.dev/2021/07/why-some-prometheus-alerts-in-k8s-can-confuse/</id><updated>2021-07-30T00:00:00Z</updated><content type="html">Recently I was installing a kubernetes cluster as I usually do for my tests. However, as those machines were bare metal servers that some colleagues have recycled, we decided to keep it running and try to maintain it by ourselves. First thing I did was a simple bot to send the alerts to a telegram channel. That was something I did not do in the past, because I do not care about monitoring as my clusters were ephemeral. After two days, an alert started firing. This blog is my analysis of what this alert means and why I consider it not accurate, so I ended silencing it (but I would love to have an accurate alert for that situation). THE ALERT The alert in question is KubeMemoryOvercommit. It is defined in this , and its description says: Cluster has overcommitted memory resource requests for Pods and cannot tolerate node failure. The rule expression for this alert is sum(namespace_memory:kube_pod_container_resource_requests:sum{}) / sum(kube_node_status_allocatable{resource="memory"}) &gt; ((count(kube_node_status_allocatable{resource="memory"}) &gt; 1) - 1) / count(kube_node_status_allocatable{resource="memory"}) Let’s try to understand what it means. THE CALCULATIONS Looking at the different members of the equation, we can see what it means: sum(namespace_memory:kube_pod_container_resource_requests:sum{}) = sum of memory requests in bytes for all pods in every namespace sum(kube_node_status_allocatable{resource="memory"}) = sum of memory that can be allocated to pods in every nodeernel Dividing both, we obtain the ratio of memory allocated related with the total amount that can be. Let’s go for the second part. count(kube_node_status_allocatable{resource="memory"}) = number of nodes that can be used to deploy pods ((count(kube_node_status_allocatable{resource="memory"}) &gt; 1) - 1) = number of nodes that can be used to deploy pods minus one, which should be at least one = remaining nodes which can allocate pods in case one is down Dividing the latter by the former, we got a ratio of (nodes - 1)/nodes Having the details in mind, a summary of this equation is It seems very easy to understand, but it is accurate? Let’s see what it is not in the formula. WHAT IS NOT CONSIDERED? I will try to summarize some of my thoughts in bullets. For the first part, to know if pods of a node can be reallocatable or not, there are lots of different concepts to bear in mind: * Daemonsets deploys pods per node, and this pod is meant for that node. It cannot be moved. * NodeSelector can be used to restrict which nodes a pod can land into. * Affinity/anti-affinity rules can also restrict where pods can be deployed * Taints and tolerations can avoid pods to be scheduled in those nodes. * Special hardware requirements, for example GPUs, are also restricted to some nodes of the cluster. Rules for deploying use to be based on For the second part of the equation: * Nodes can be different in size, so the second part of the equation is not accurate either. For example, if we have 1 nodes of 512 Gb RAM and 1 of 64 Gb RAM, if you have one half full and one empty (which you can see it will not fire the alert), if the full node is the bigger one and it goes down, you cannot reallocate the pods in the smaller, but it will be possible on the opposite direction. CONCLUSION I find it very useful to know if the platform has any risk of overcommitting the workloads if a node shuts down (either abruptly or planned). I find it particularly relevant in upgrades, where nodes usually have to be rebooted during the upgrade in a rolling process. However, as I have shown previously, there are so many factors to consider than a simple rule based on a couple of metrics gives a false security feeling. The only “accurate” solution that comes to my mind is if we can tell the scheduler to simulate the drain of a node and throw any issues that could be foreseen, like an extension of what is with kubectl drain --dry-run&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FDn2otUDpuU" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://blog.ramon-gordillo.dev/2021/07/why-some-prometheus-alerts-in-k8s-can-confuse/</feedburner:origLink></entry><entry><title>Troubleshooting application performance with Red Hat OpenShift metrics, Part 4: Gathering performance metrics</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/3oYKYRb3wLU/troubleshooting-application-performance-red-hat-openshift-metrics-part-4" /><author><name>Pavel Macik</name></author><id>66873ac1-84f5-4f2b-83ad-fb28e2a5b5a7</id><updated>2021-07-29T07:00:00Z</updated><published>2021-07-29T07:00:00Z</published><summary type="html">&lt;p&gt;This series shows how to use &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; metrics in a real-life performance testing scenario. I used these metrics to run performance tests on the &lt;a href="https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator"&gt;Service Binding Operator&lt;/a&gt;. We used the results to performance-tune the Service Binding Operator for acceptance into the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3"&gt;Part 3&lt;/a&gt;, I showed you how we set up &lt;a href="https://docs.openshift.com/container-platform/4.7/monitoring/understanding-the-monitoring-stack.html?extIdCarryOver=true&amp;intcmp=7013a000002w3nnAAA&amp;sc_cid=7013a000002w0ZmAAI"&gt;OpenShift's monitoring stack&lt;/a&gt; to collect runtime metrics for our testing scenarios. I also shared a collector script that ensures the results are preserved on a node that won't crash. Now, we can look at the performance metrics we'll use and how to gather the data we need.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the whole series&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1"&gt;Performance requirements&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2021/07/15/troubleshooting-application-performance-red-hat-openshift-metrics-part-2-test"&gt;The test environment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3"&gt;Collecting runtime metrics&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Part 4: Gathering performance metrics&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Part 5: Test rounds and results (August 5)&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;CPU and memory usage&lt;/h2&gt; &lt;p&gt;As mentioned in &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3"&gt;Part 3&lt;/a&gt;, some metrics have to be collected for the duration of the test. The most important metrics required by the Developer Sandbox team were the CPU and memory usage of both OpenShift nodes and the tested operators. We also wanted to see the number of resources left in the cluster after the test was done.&lt;/p&gt; &lt;p&gt;Unfortunately, we ran into problems during our first attempts to run the stress tests. The OpenShift cluster actually crashed when one of its worker nodes came down. Naturally, it was important to know what caused the failure. I needed a more granular view of the CPU and memory usage, so I had to collect data from the workloads deployed on those nodes.&lt;/p&gt; &lt;p&gt;From watching and inspection using the cluster's own &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; instance, I identified a couple of resources that were loaded and stressed by our scenario. Then, I included them to be watched by the &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3#collecting_runtime_metrics_with_the_openshift_monitoring_tool"&gt;collector script&lt;/a&gt; I shared in the previous article, together with the cluster's nodes.&lt;/p&gt; &lt;p&gt;I identified workloads from a handful of namespaces as stressed, and included them to be watched for CPU and memory usage. Table 1 shows these namespaces and workloads.&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="500"&gt;&lt;caption&gt;Table 1: Workloads to watch for CPU and memory usage.&lt;/caption&gt; &lt;tbody&gt;&lt;tr&gt;&lt;th scope="col"&gt;Namespace&lt;/th&gt; &lt;th scope="col"&gt;Workloads&lt;/th&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-apiserver&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;apiserver-*&lt;/code&gt; pods&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-kube-apiserver&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;kube-apiserver-*&lt;/code&gt; pods&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-monitoring&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;prometheus-k8s-*&lt;/code&gt; pods&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-operators&lt;/code&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;service-binding-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;li&gt;&lt;code&gt;rhoas-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;code&gt;openshift-operator-lifecycle-manager&lt;/code&gt;&lt;/td&gt; &lt;td&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;catalog-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;li&gt;&lt;code&gt;olm-operator-*&lt;/code&gt; pods&lt;/li&gt; &lt;/ul&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;h2&gt;Resources created in the cluster&lt;/h2&gt; &lt;p&gt;Another one of the metrics requested by the Developer Sandbox team was the number of resources created in the cluster during the test. There were two ways to get that information:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;If the Prometheus instance was available (meaning the node on which it is deployed had not crashed), I could use a simple Prometheus query: &lt;pre&gt; &lt;code&gt;sort_desc(cluster:usage:resources:sum)&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;If the Prometheus instance was not available, I had to employ a brute-force approach, using the &lt;code&gt;oc&lt;/code&gt; tool to count the number of each resource.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Service Binding Operator performance and developer experience&lt;/h2&gt; &lt;p&gt;The final set of metrics was related to the performance of the Service Binding Operator itself, from a developer's perspective. Specifically, we wanted to know how long it took to perform the binding after the &lt;code&gt;ServiceBinding&lt;/code&gt; resource was created.&lt;/p&gt; &lt;p&gt;The typical situation for a developer using the Service Binding Operator is to have a backing service and an application running that the user wants to bind together. So, the developer sends a &lt;code&gt;ServiceBinding&lt;/code&gt; request and expects the binding to be done by Service Binding Operator. The scenario can be split into the following sequence of steps. Each step is shown along with the way to retrieve the respective timestamp:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;The &lt;code&gt;ServiceBinding&lt;/code&gt; request is sent, processed by OpenShift, and created internally as a resource (&lt;code&gt;.metadata.creationTimestamp&lt;/code&gt; of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource).&lt;/li&gt; &lt;li&gt;The Service Binding Operator picks up the resource while watching for it and processes it. (This is the first "Reconciling ServiceBinding" message for the particular &lt;code&gt;ServiceBinding&lt;/code&gt; resource in the Service Binding Operator logs.)&lt;/li&gt; &lt;li&gt;Based on the content of the resource, the Service Binding Operator performs the binding. It collects bindable information from the backing service and injects it into the application.&lt;/li&gt; &lt;li&gt;The application is re-deployed with the bound information injected into the &lt;code&gt;Deployment&lt;/code&gt; resource (&lt;code&gt;.status.conditions[] | select(.type=="Available") | select(.status=="True").lastTransitionTime&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;The &lt;code&gt;ServiceBinding&lt;/code&gt; resource is marked as done. (A "Done" message is sent for the particular &lt;code&gt;ServiceBinding&lt;/code&gt; resource in the Service Binding Operator logs.)&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;I defined the following metrics to evaluate the developer experience in this scenario:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Time to Ready&lt;/strong&gt;: The time between the creation of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource (1) and completion of the binding (5). That can be further split into the following: &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Time to Reconcile&lt;/strong&gt;: The time between the creation of the &lt;code&gt;ServiceBinding&lt;/code&gt; resource (1) and when it is picked up by Service Binding Operator (2).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reconcile to Ready&lt;/strong&gt;: The time between when the Service Binding Operator picks up the &lt;code&gt;ServiceBinding&lt;/code&gt; (2) and completes the binding (5).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;At the time of the performance evaluation, these metrics were not collected by OpenShift's monitoring stack or by the Service Binding Operator. So, I had to dig up the information from the data that I could get. As shown earlier, I derived the information I needed from metadata that OpenShift collects about the active user's backing service and the application (especially timestamps), along with the Service Binding Operator logs. I wrote the following script to collect the necessary data from OpenShift and compute the information after the test is complete:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;if [ -z "$QUAY_NAMESPACE" ]; then echo "QUAY_NAMESPACE environemnt variable needs to be set to a non-empty value" exit 1 fi DT=$(date "+%F_%T") RESULTS=results-$DT mkdir -p $RESULTS USER_NS_PREFIX=${1:-zippy} # Resource counts resource_counts(){ echo -n "$1;" # All resource counts from user namespaces echo -n "$(oc get $1 --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace | grep $USER_NS_PREFIX | wc -l)" echo -n ";" # All resource counts from all namespaces echo "$(oc get $1 --all-namespaces -o name | wc -l)" } # Dig various timestamps out timestamps(){ SBR_JSON=$1 DEPLOYMENTS_JSON=$2 SBO_LOG=$3 RESULTS=$4 jq -rc '((.metadata.namespace) + ";" + (.metadata.name) + ";" + (.metadata.creationTimestamp) + ";" + (.status.conditions[] | select(.type=="Ready").lastTransitionTime))' $SBR_JSON &gt; $RESULTS/tmp.csv echo "ServiceBinding;Created;ReconciledTimestamp;Ready;AllDoneTimestamp" &gt; $RESULTS/sbr-timestamps.csv for i in $(cat $RESULTS/tmp.csv); do ns=$(echo -n $i | cut -d ";" -f1) name=$(echo -n $i | cut -d ";" -f2) echo -n $ns/$name; echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f3) "+%F %T"); echo -n ";"; log=$(cat $SBO_LOG | grep $ns) date -d @$(echo $log | jq -rc 'select(.msg | contains("Reconciling")).ts' | head -n1) "+%F %T.%N" | tr -d "\n" echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f4) "+%F %T"); echo -n ";"; done_ts=$(echo $log | jq -rc 'select(.msg | contains("Done")) | select(.retry==false).ts') if [ -n "$done_ts" ]; then date -d "@$done_ts" "+%F %T.%N" else echo "" fi done &gt;&gt; $RESULTS/sbr-timestamps.csv rm -f $RESULTS/tmp.csv jq -rc '((.metadata.namespace) + ";" + (.metadata.name) + ";" + (.metadata.creationTimestamp) + ";" + (.status.conditions[] | select(.type=="Available") | select(.status=="True").lastTransitionTime)) + ";" + (.metadata.managedFields[] | select(.manager=="manager").time)' $DEPLOYMENTS_JSON &gt; $RESULTS/tmp.csv echo "Namespace;Deployment;Deployment_Created;Deployment_Available;Deployment_Updated_by_SBO;SB_Name;SB_created;SB_ReconciledTimestamp;SB_Ready;SB_AllDoneTimestamp" &gt; $RESULTS/binding-timestamps.csv for i in $(cat $RESULTS/tmp.csv); do NS=$(echo -n $i | cut -d ";" -f1); echo -n $NS; echo -n ";"; echo -n $(echo -n $i | cut -d ";" -f2); echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f3) "+%F %T"); echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f4) "+%F %T"); echo -n ";"; echo -n $(date -d $(echo -n $i | cut -d ";" -f5) "+%F %T"); echo -n ";"; cat $RESULTS/sbr-timestamps.csv | grep $NS done &gt;&gt; $RESULTS/binding-timestamps.csv rm -f $RESULTS/tmp.csv } # Collect timestamps { # ServiceBinding resources in user namespaces oc get sbr --all-namespaces -o json | jq -r '.items[] | select(.metadata.namespace | contains("'$USER_NS_PREFIX'"))' &gt; $RESULTS/service-bindings.json # Deployment resources in user namespaces oc get deploy --all-namespaces -o json | jq -r '.items[] | select(.metadata.namespace | contains("'$USER_NS_PREFIX'"))' &gt; $RESULTS/deployments.json # ServiceBiding operator log oc logs $(oc get $(oc get pods -n openshift-operators -o name | grep service-binding-operator) -n openshift-operators -o jsonpath='{.metadata.name}') -n openshift-operators &gt; $RESULTS/service-binding-operator.log timestamps $RESULTS/service-bindings.json $RESULTS/deployments.json $RESULTS/service-binding-operator.log $RESULTS } &amp; # Collect resource counts { RESOURCE_COUNTS_OUT=$RESULTS/resource-count.csv echo "Resource;UserNamespaces;AllNamespaces" &gt; $RESOURCE_COUNTS_OUT for i in $(cat resources.list); do resource_counts $i &gt;&gt; $RESOURCE_COUNTS_OUT; done } &amp; wait &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;This article introduced the metrics we collected to performance-test the Service Binding Operator for acceptance into the Developer Sandbox for Red Hat OpenShift. I showed how we collected both metrics of interest to the Developer Sandbox team, and additional metrics for our team specifically. Look for Part 5, the final article in this series, where I will present the testing rounds and their results.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/29/troubleshooting-application-performance-red-hat-openshift-metrics-part-4" title="Troubleshooting application performance with Red Hat OpenShift metrics, Part 4: Gathering performance metrics"&gt;Troubleshooting application performance with Red Hat OpenShift metrics, Part 4: Gathering performance metrics&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/3oYKYRb3wLU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Pavel Macik</dc:creator><dc:date>2021-07-29T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/29/troubleshooting-application-performance-red-hat-openshift-metrics-part-4</feedburner:origLink></entry><entry><title type="html">Separating RESTEasy Spring And Microprofile Components Into Independent Subprojects.</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/o1oznQA48yA/" /><author><name /></author><id>https://resteasy.github.io/2021/07/29/separate-spring-and-microprofile/</id><updated>2021-07-29T00:00:00Z</updated><dc:creator /><summary type="html">&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/o1oznQA48yA" height="1" width="1" alt=""/&gt;</summary><feedburner:origLink>https://resteasy.github.io/2021/07/29/separate-spring-and-microprofile/</feedburner:origLink></entry><entry><title>Write toolchain-agnostic RPM spec files for GCC and Clang</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6Vh3_fS-NoA/write-toolchain-agnostic-rpm-spec-files-gcc-and-clang" /><author><name>Timm Baeder</name></author><id>646d4267-7a40-4821-969a-2740bffb8c0e</id><updated>2021-07-28T07:00:00Z</updated><published>2021-07-28T07:00:00Z</published><summary type="html">&lt;p&gt;With the addition of the &lt;code&gt;%toolchain&lt;/code&gt; macro to the &lt;code&gt;redhat-rpm-config&lt;/code&gt; package, packages can easily switch between the &lt;a href="https://gcc.gnu.org/"&gt;GNU Compiler Collection&lt;/a&gt; (GCC) and the &lt;a href="https://clang.llvm.org/"&gt;Clang compiler&lt;/a&gt;. This package change is not yet supported by Fedora, and package maintainers need good reasons to switch from the GCC default to Clang. Maintainers also need to watch out for a few nuances to make (and keep) a package specification file buildable with both toolchains.&lt;/p&gt; &lt;p&gt;This article looks at the necessary changes and best practices to allow a spec file to build with both GCC and Clang in a variety of cases.&lt;/p&gt; &lt;h2&gt;Preparing a spec file&lt;/h2&gt; &lt;p&gt;In the most basic case, nothing needs to be done. The specification file will automatically pick up whatever compiler the buildroot defines as the default. You can check the default by looking at the value of the &lt;code&gt;%toolchain&lt;/code&gt; macro in the &lt;code&gt;%build&lt;/code&gt; section:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build echo "Toolchain is %toolchain"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Similarly, &lt;code&gt;rpm --eval "%toolchain"&lt;/code&gt; will print the value of the &lt;code&gt;%toolchain&lt;/code&gt; macro. Setting the macro to either "&lt;code&gt;gcc&lt;/code&gt;" or "&lt;code&gt;clang&lt;/code&gt;" will select the toolchain for the given spec file. Setting the macro explicitly is useful if the package needs to be built with a specific toolchain:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%global toolchain clang&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For testing, it might make sense to add a conditional variable to the spec file, so it can be easily built with either toolchain:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;%bcond_with toolchain_clang %if %{with toolchain_clang} %global toolchain clang %else %global toolchain gcc %endif&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Adding the conditional variable should suffice for the minimal setup. In some cases, you might need to make additional changes such as adjusting the &lt;code&gt;BuildRequires&lt;/code&gt;. This is because the &lt;code&gt;%toolchain&lt;/code&gt; macro does not automatically add the appropriate toolchain &lt;code&gt;BuildRequires&lt;/code&gt;. With this setup, building a simple RPM via &lt;code&gt;rpmbuild --with=toolchain_clang&lt;/code&gt; will select the Clang toolchain while not passing anything or passing &lt;code&gt;--with=gcc&lt;/code&gt; will select GCC.&lt;/p&gt; &lt;p&gt;The value of the &lt;code&gt;%toolchain&lt;/code&gt; macro will later determine the value of the various environment variables, as well as what build flags will be passed to the build system. So this macro should be set as early as possible in the file.&lt;/p&gt; &lt;h2&gt;Build systems&lt;/h2&gt; &lt;p&gt;Most C and C++ software projects use a popular build system such as Autotools, CMake, or Meson. These build systems usually try to follow standard practices such as respecting common environment variables, e.g., &lt;code&gt;CC&lt;/code&gt;, &lt;code&gt;CXX&lt;/code&gt;, or &lt;code&gt;CFLAGS&lt;/code&gt;. They are readily supported by RPM via their respective macros, such as &lt;code&gt;%cmake&lt;/code&gt; for CMake, &lt;code&gt;%configure&lt;/code&gt; for Autotools, and &lt;code&gt;%meson&lt;/code&gt; for Meson.&lt;/p&gt; &lt;p&gt;After setting those macros, use &lt;code&gt;%make_build&lt;/code&gt; to actually compile the project. This macro will run &lt;code&gt;make&lt;/code&gt; and pass a few flags for parallel builds. There are special macros to let CMake and Meson build the application, in case the build uses Ninja instead of &lt;code&gt;make&lt;/code&gt;: &lt;code&gt;%cmake_build&lt;/code&gt; and &lt;code&gt;%meson_build&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Just using these macros works for the simplest projects. Some projects, however, do not use any of the common build systems and require special care.&lt;/p&gt; &lt;h2&gt;Hand-written makefiles&lt;/h2&gt; &lt;p&gt;Hand-written makefiles are still relatively common, especially in smaller software projects. Unfortunately, makefiles do not usually respect the environment variables mentioned earlier. This makes it impossible for distributions to inject their hardening C flags or change the compiler in use.&lt;/p&gt; &lt;p&gt;When using hand-written makefiles in a Fedora RPM spec file, the &lt;code&gt;%make_build&lt;/code&gt; macro works as well. However, that does not define the &lt;code&gt;CFLAGS&lt;/code&gt; environment variable and others. &lt;code&gt;%set_build_flags&lt;/code&gt; can be used to set those flags in the makefile:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags %make_build&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;%set_build_flags&lt;/code&gt; also sets the &lt;code&gt;CC&lt;/code&gt; environment variable, so if the compiler needs to be invoked explicitly, &lt;code&gt;$CC&lt;/code&gt; should be used instead of hard-coding &lt;code&gt;gcc&lt;/code&gt; or &lt;code&gt;clang&lt;/code&gt;, or even &lt;code&gt;/usr/bin/cc&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If the provided makefile respects the standard environment variables, you are all set with this approach. If it doesn't, it usually needs to be patched, or the spec file needs to use another environment variable, for example &lt;code&gt;EXTRA_CFLAGS&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags %make_build EXTRA_CFLAGS="$CFLAGS"&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Custom build setups&lt;/h2&gt; &lt;p&gt;This is more of a worst-case scenario for packagers, but many projects still have custom build setups (for example, a shell script that compiles the source code) or no build setup at all.&lt;/p&gt; &lt;p&gt;If the project does not have any build setup, it's easiest to simply use &lt;code&gt;%set_build_flags&lt;/code&gt; again and compile using &lt;code&gt;$CC&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags # Compile main library $CC lib.c -c -o lib.o $CFLAGS $LDFLAGS&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, if the project uses a custom shell script (or something similar), you're out of luck again. Using &lt;code&gt;%set_build_flags&lt;/code&gt; is still a good idea, but you have to inspect the build script to find out what environment variables have to be set or what command-line parameters have to be passed.&lt;/p&gt; &lt;h2&gt;Toolchain-specific changes&lt;/h2&gt; &lt;p&gt;One common problem that spec files run into is that they want to add (or override) flags from the &lt;code&gt;CFLAGS&lt;/code&gt; variable to keep a working build. That is of course easy to do:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags CFLAGS="$CFLAGS -fno-some-flag"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, we assume that a new version of the compiler in use has introduced a new flag that's enabled by default, &lt;code&gt;-fsome-flag&lt;/code&gt;. This flag causes problems when the project in question is compiled, however, so the package maintainer has decided to disable it again by passing &lt;code&gt;-fno-some-flag&lt;/code&gt;. When trying to build this package with a different compiler, it can happen that the build fails because the compiler does not know about &lt;code&gt;-fsome-flag&lt;/code&gt; or &lt;code&gt;-fno-some-flag&lt;/code&gt;. When &lt;code&gt;-fno-some-flag&lt;/code&gt; needs to be passed only for a specific compiler, check the &lt;code&gt;%toolchain&lt;/code&gt; macro for the compiler that requires the flag:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%build %set_build_flags %if "%toolchain" == "gcc" CFLAGS="$CFLAGS -fno-some-flag" %endif&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As implemented in &lt;code&gt;redhat-rpm-macros&lt;/code&gt;, the &lt;code&gt;%toolchain&lt;/code&gt; macro always evaluates to either "gcc" or "clang." Note that one cannot generally check &lt;code&gt;$CC&lt;/code&gt; the same way, because that variable might point to an obscure binary in cross-compilation scenarios.&lt;/p&gt; &lt;h2&gt;Link-time optimization with Clang&lt;/h2&gt; &lt;p&gt;Link-time optimization (LTO) is enabled by default on Fedora, which means that the &lt;code&gt;$CFLAGS&lt;/code&gt; variable (set by &lt;code&gt;%set_build_flags&lt;/code&gt;) contains a variation of the &lt;code&gt;-flto&lt;/code&gt; flag. The flags used by link-time optimization are saved in the &lt;code&gt;%_lto_cflags&lt;/code&gt; variable. When porting a package to compile with the Clang toolchain, it is important to know that the link-time optimization flags have to be added to the &lt;code&gt;$LDFLAGS&lt;/code&gt; variable as well. &lt;code&gt;%set_build_flags&lt;/code&gt; will take care of that.&lt;/p&gt; &lt;p&gt;When the package needs some non-standard variable, the flags from &lt;code&gt;%_lto_cflags&lt;/code&gt; need to be added manually. In the worst case, link-time optimization has to be disabled entirely. This change is often necessary when the project has some home-grown LTO setup that expects GCC. Disabling link-time optimization can be done by setting &lt;code&gt;%_lto_cflags&lt;/code&gt; to &lt;code&gt;%{nil}&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;%global _lto_cflags %{nil}&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Supporting both the GCC and Clang toolchains is easy if the package uses a standard build system and respects standard environment variables. There are still tons of special cases, which can often be handled by checking the &lt;code&gt;%toolchain&lt;/code&gt; macro value and handling the problem depending on the toolchain in use.&lt;/p&gt; &lt;p&gt;Note that currently (as of Fedora 34) all packages in Fedora are built with GCC, but there is a proposal for changing this. See &lt;a href="https://fedoraproject.org/wiki/Changes/CompilerPolicy"&gt;Fedora's Changes/CompilerPolicy page&lt;/a&gt; for details.&lt;/p&gt; &lt;p&gt;Good documentation for these matters is scarce, but I like to look at the macros defined in the &lt;a href="https://github.com/rpm-software-management/rpm/blob/master/macros.in"&gt;RPM repository&lt;/a&gt; as well as the &lt;a href="https://src.fedoraproject.org/rpms/redhat-rpm-config/blob/rawhide/f/macros"&gt;redhat-rpm-config repository&lt;/a&gt;. Looking at the output of &lt;code&gt;rpm --showrc&lt;/code&gt; can also provide insight.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/28/write-toolchain-agnostic-rpm-spec-files-gcc-and-clang" title="Write toolchain-agnostic RPM spec files for GCC and Clang"&gt;Write toolchain-agnostic RPM spec files for GCC and Clang&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6Vh3_fS-NoA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Timm Baeder</dc:creator><dc:date>2021-07-28T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/28/write-toolchain-agnostic-rpm-spec-files-gcc-and-clang</feedburner:origLink></entry><entry><title type="html">Importing and Reusing DMN Models in Red Hat Decision Manager</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2oCeTPPJwg4/reusing-dmn-models.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/07/reusing-dmn-models.html</id><updated>2021-07-28T01:52:40Z</updated><content type="html">Decision Model and Notation (DMN) is a standard established by the Object Management Group (OMG) for describing and modeling operational decisions. DMN decision models can be shared between DMN-compliant platforms and across organizations so that business analysts and business rules developers are unified in designing and implementing DMN decision services. Reusing a DMN model in the scope of a larger decision flow is a common requirement.  did a  earlier this year on this topic. In this presentation, he walks through how we can achieve model reuse through the concept of DMN Decision Service. This article explains the same concept in a step by step manner with a sample use case. A decision service is an invocable function, with well-defined inputs and outputs, that is published as a service for invocation. The decision service can be invoked similarly to a Business Knowledge Model (BKM) node from within the model itself if necessary. The standard usage of the decision service node is for it to be leveraged for re-use from an external application or a business process. The decision service allows for reusability by providing an interface for invocation from another DMN model. To know more about how to implement reusable decisions, check out for a step by step explanation with an example. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2oCeTPPJwg4" height="1" width="1" alt=""/&gt;</content><dc:creator>Sadhana Nandakumar</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/reusing-dmn-models.html</feedburner:origLink></entry><entry><title type="html">WildFly 24.0.1 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VmOJ2VFEFtM/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2021/07/28/WildFly2401-Released/</id><updated>2021-07-28T00:00:00Z</updated><content type="html">WildFly 24.0.1.Final is now available . It’s been about six weeks since the WildFly 24 release, so we’ve done a small bug fix update, WildFly 24.0.1. This includes an update to WildFly Preview. The full list of issues resolved in WildFly 24.0.1 is available . Issues resolved in the WildFly Core update included with WildFly 24.0.1 are available . Enjoy!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VmOJ2VFEFtM" height="1" width="1" alt=""/&gt;</content><dc:creator>Brian Stansberry</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/07/28/WildFly2401-Released/</feedburner:origLink></entry><entry><title type="html">Using TrustyAI’s explainability from Python</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/QTmRHgWcSTw/using-trustyais-explainability-from-python.html" /><author><name>Rui Vieira</name></author><id>http://feeds.athico.com/~r/droolsatom/~3/T0YquR_VYkE/using-trustyais-explainability-from-python.html</id><updated>2021-07-27T17:15:06Z</updated><content type="html">The ‘s explainability library is primarily aimed at the Java Virtual Machine (JVM) and designed to be integrated seamlessly with the remaining TrustyAI services, adding explainability capabilities (such as feature importance and counterfactual explanations) to business automation workflows that integrate predictive models. Many of these capabilities are useful on their own. However, in the data science field, Java is not usually the first language of choice, with Python being the most likely choice. In this blog post, I will show a quick example of how to use TrustyAI’s explainability library from Python, allowing to create quick prototypes, using it from Jupyter notebooks or integrating with the wider Python ecosystem. To do so, we will use the Python library, which creates low-level bindings to the JVM and provides wrappers to Java-specific types, allowing us to seamlessly communicate with the explainability library and call the available methods in a "Pythonic" way. At the moment you can try all the examples in this post in two different ways: * Install the python-trustyai library locally, by cloning the repository and installing with python setup.py install * Build a container using the manifest provided in the repository (or using it as a base for your own) SETTING UP DEPENDENCIES The library will execute code directly from the JARs, so the first step after importing it is to initialize the bindings by providing a "classpath". You can specify the JARs locations relative to your script (for convenience a shell script is provided which will download the dependencies from Maven Central). For instance: import trustyai ​ trustyai.init( path=[ "../dep/org/kie/kogito/explainability-core/1.8.0.Final/*", # any other dependencies ] ) COUNTERFACTUAL EXPLANATIONS We will start by showing how to search for counterfactual explanations using a toy model. If you are not familiar with counterfactuals, the simplest definition is as follows: Assuming you have a predictive model and an original input, but those inputs do not provide the outcome you want. The counterfactual will be an alternative input (as close as possible to the original) that has the desired outcome. As an example, a counterfactual for a loan approval predictive model could be expressed as "my application was rejected, but if my application data was this, then it would have been approved". (For a more detailed introduction to counterfactuals you can watch the” KIE group video.) USING A TOY MODEL TrustyAI’s explainability methods are aimed primarily at black-box models, that is models for which we are unaware of the internals and can only interact by sending inputs and receiving predictions. We will define a toy model in order to introduce some basic concepts, and later on, we will look at how to integrate this library with a more complex, Python-based model. This model takes an all-numerical input x and returns a y of either true or false if the sum of the x components is within a threshold e of a point C This model is provided in the TestUtils python-trustyai module. We simply import it from Python and initialise it with C=500 and e=1.0. from trustyai.utils import TestUtils ​ center = 500.0 epsilon = 1.0 ​ model = TestUtils.getSumThresholdModel(center, epsilon) Next, we need to define a goal or desired outcome. We will define it as a feature with the value true, that is, we want the sum of the input features to be within the vicinity of a (to be defined) point C . The goal is a list of Output that take the following feature values: * The name * The type * The value (wrapped in Value) * A confidence threshold, which we will leave at zero (no threshold) from trustyai.model import Output, Type, Value ​ goal = [Output("inside", Type.BOOLEAN, Value(True), 0.0)] We will now define our initial features, x. Each feature can be instantiated by using the utility class FeatureFactory and in this case, we want to use numerical features, so we’ll use FeatureFactory.newNumericalFeature and create four features (an arbitrary number, just for example purposes): import random from trustyai.model import FeatureFactory ​ features = [ FeatureFactory.newNumericalFeature(f"x{i+1}", random.random() * 10.0) for i in range(4) ] As we can see, the sum of of the features will not be within e (1.0) of C (500.0). As such the model prediction will be false: feature_sum = 0.0 for f in features: value = f.getValue().asNumber() print(f"Feature {f.getName()} has value {value}") feature_sum += value print(f"\nFeatures sum is {feature_sum}") Feature x1 has value 5.011623817323953 Feature x2 has value 4.574121526021039 Feature x3 has value 4.240726704569074 Feature x4 has value 3.2819942125458788 Features sum is 17.108466260459945 An important concept in counterfactual is one of the constraints. A constrained feature is one that shouldn’t change its value relative to the original. This might be crucial for several reasons, such as a business constraint that wouldn’t make sense to change, a feature that would be impossible to change in the real world or even for legal reasons. Since this is just a trivial model, we will allow all features to change so we specify a list of False values, each entry corresponding to the above features. constraints = [False] * 4 Another important concept is the one of bounds or search domains. This will inform the counterfactual explainer of the limits of the search. In some cases, we can use domain-specific knowledge (for instance we know which is the sensible range of a numerical variable) or we could use the bounds to limit the counterfactual to a region of interest. Additionally, they can also be taken from the original data, if available. In this case, we simply specify an arbitrary (but sensible) value, e.g. all the features can vary between 0 and 1000. Since all the features are numerical we will use the utility method NumericalFeatureDomain.create(min, max). from trustyai.model.domain import NumericalFeatureDomain ​ feature_boundaries = [NumericalFeatureDomain.create(0.0, 1000.0)] * 4 We can now instantiate the explainer itself. We will configure the termination criteria. For this example, we will specify that the counterfactual search should only execute a maximum of 10,000 iterations before stopping and returning whatever the best result is, so far. from org.optaplanner.core.config.solver.termination import TerminationConfig from org.kie.kogito.explainability.local.counterfactual import ( CounterfactualConfigurationFactory, ) from java.lang import Long ​ termination_config = TerminationConfig().withScoreCalculationCountLimit( Long.valueOf(10_000) ) ​ solver_config = ( CounterfactualConfigurationFactory.builder() .withTerminationConfig(termination_config) .build() ) We can now instantiate the explainer itself using CounterfactualExplainer and our solver_config configuration. from trustyai.explainers import CounterfactualExplainer ​ explainer = CounterfactualExplainer(solver_config) We will now express the counterfactual problem as defined above. * original represents our x * goals, that is our desired prediction (True) * A domain represents the boundaries for the counterfactual search and wrap these quantities in a CounterfactualPrediction (the UUID is simply to uniquely label the search instance): import uuid from trustyai.model import PredictionFeatureDomain, PredictionInput, PredictionOutput, CounterfactualPrediction original = PredictionInput(features) goals = PredictionOutput(goal) domain = PredictionFeatureDomain(feature_boundaries) prediction = CounterfactualPrediction( original, goals, domain, constraints, None, uuid.uuid4() ) We now request the counterfactual x’ which is closest to x and which satisfies f(x’,e,C)=y’: explanation = explainer.explain(prediction, model) Let’s look at the resulting counterfactual x’: feature_sum = 0.0 for entity in explanation.getEntities(): print(entity) feature_sum += entity.getProposedValue() ​ print(f"\nFeature sum is {feature_sum}") java.lang.DoubleFeature{value=5.011623817323953, intRangeMinimum=0.0, intRangeMaximum=1000.0, id='x1'} java.lang.DoubleFeature{value=4.160541846083166, intRangeMinimum=0.0, intRangeMaximum=1000.0, id='x2'} java.lang.DoubleFeature{value=4.240726704569074, intRangeMinimum=0.0, intRangeMaximum=1000.0, id='x3'} java.lang.DoubleFeature{value=485.6288805328477, intRangeMinimum=0.0, intRangeMaximum=1000.0, id='x4'} Feature sum is 499.0417729008239 We can see that the explainer found a valid counterfactual for this model and input since the sum of the features is within e=1.0 of C=500.0. As we’ve discussed, it is possible to constraint a specific feature xi by setting the constraints list corresponding element to True. In this example, we now want to fix x1 and x4 and see if the result is as expected. That is, these features should have the same value in the counterfactual x’ as in the original x. constraints = [True, False, False, True] # x1, x2, x3 and x4 We simply need to wrap the previous quantities with the new constraints and request a new counterfactual explanation: prediction = CounterfactualPrediction( original, goals, domain, constraints, None, uuid.uuid4() ) explanation = explainer.explain(prediction, model) We can see that x1 and x4 have the same value as the original and the model satisfies the conditions. print(f"Original x1: {features[0].getValue()}") print(f"Original x4: {features[3].getValue()}\n") ​ for entity in explanation.getEntities(): print(entity) Original x1: 5.011623817323953 Original x4: 3.2819942125458788 java.lang.DoubleFeature{value=5.011623817323953, intRangeMinimum=5.011623817323953, intRangeMaximum=5.011623817323953, id='x1'} java.lang.DoubleFeature{value=4.574121526021039, intRangeMinimum=0.0, intRangeMaximum=1000.0, id='x2'} java.lang.DoubleFeature{value=486.268546111066, intRangeMinimum=0.0, intRangeMaximum=1000.0, id='x3'} java.lang.DoubleFeature{value=3.2819942125458788, intRangeMinimum=3.2819942125458788, intRangeMaximum=3.2819942125458788, id='x4'} USING PYTHON MODELS We’ve covered some basic concepts with the previous toy model but now will look at a slightly different example. We will now show how to use a custom (and more complex) Python model with TrustyAI counterfactual explanations. This model will predict how likely a loan is to be repaid given the applicant characteristics. The model used was, trained with a considerably large, anonymised and public dataset of real financial institutions. This is, undoubtedly, closer to a real-world scenario than our toy model. For convenience and brevity, the model training steps are not included and the model was serialised using the joblib library so that for this example we simply need to deserialize it for it to be ready to use. import joblib ​ xg_model = joblib.load("models/credit-bias-xgboost.joblib") print(xg_model) XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1, importance_type='gain', interaction_constraints='', learning_rate=0.07, max_delta_step=0, max_depth=8, min_child_weight=1, missing=nan, monotone_constraints='()', n_estimators=200, n_jobs=12, num_parallel_tree=1, random_state=27, reg_alpha=0, reg_lambda=1, scale_pos_weight=0.9861206227457426, seed=27, subsample=1, tree_method='exact', validate_parameters=1, verbosity=None) This model has as a single output a boolean PaidLoan, which, as mentioned, will contain the prediction of whether a certain loan applicant will repay the loan in time or not. The model is slightly more complex than the previous examples, with considerably more input features. We will start by testing the model with an input we are quite sure (from the original data) that will be predicted as false: x = [ False, # NewCreditCustomer 2125.0, # Amount 20.97, # Interest 60.0, # LoanDuration 4.0, # Education 0.0, # NrOfDependants 6.0, # EmploymentDurationCurrentEmployer 0.0, # IncomeFromPrincipalEmployer 301.0, # IncomeFromPension 0.0, # IncomeFromFamilyAllowance 53.0, # IncomeFromSocialWelfare 0.0, # IncomeFromLeavePay 0.0, # IncomeFromChildSupport 0.0, # IncomeOther 8.0, # ExistingLiabilities 6.0, # RefinanceLiabilities 26.29, # DebtToIncome 10.92, # FreeCash 1000.0, # CreditScoreEeMini 1.0, # NoOfPreviousLoansBeforeLoan 500.0, # AmountOfPreviousLoansBeforeLoan 590.95, # PreviousRepaymentsBeforeLoan 0.0, # PreviousEarlyRepaymentsBefoleLoan 0.0, # PreviousEarlyRepaymentsCountBeforeLoan False, # Council_house False, # Homeless False, # Joint_ownership False, # Joint_tenant False, # Living_with_parents False, # Mortgage False, # Other False, # Owner True, # Owner_with_encumbrance True, # Tenant False, # Entrepreneur False, # Fully False, # Partially True, # Retiree False, # Self_employed ] The model’s prediction is that this application will not be repaid with a probability of ~77%: import numpy as np ​ print(xg_model.predict_proba(np.array(x))) print(f"Paid loan is predicted as: {xg_model.predict(np.array(x))}") [[0.7770493 0.22295067]] Paid loan is predicted as: [False] Since Python models cannot be passed directly as-is to the counterfactual explainer, we will prepare the XGBoost model to be used from the TrustyAI counterfactual engine. Fortunately, the process is simple enough. We only need to create a prediction function that takes: * A java.util.List of PredictionInput as inputs * A java.util.List of PredictionOutput as outputs If this interface is used, the actual inner working of this method can be anything (including calling a XGBoost Python model for prediction, as in our case). We will use a utility method, toJList, which converts a Python list into a Java List, leaving the contents unchanged. from typing import List from trustyai.utils import toJList ​ ​ def predict(inputs): values = [feature.getValue().asNumber() for feature in inputs.get(0).getFeatures()] result = xg_model.predict_proba(np.array([values])) false_prob, true_prob = result[0] if false_prob &gt; true_prob: prediction = (False, false_prob) else: prediction = (True, true_prob) output = Output("PaidLoan", Type.BOOLEAN, Value(prediction[0]), prediction[1]) return toJList([PredictionOutput([output])]) Once the prediction method is created, we wrap it in a PredictionProvider class. Since the TrustyAI explainability API is asynchronous (based on CompletableFutures), this class takes care of all the JVM’s asynchronous plumbing for us. from trustyai.model import PredictionProvider ​ model = PredictionProvider(predict) We will now express the previous inputs (x) in terms of Features, so that we might use it for the counterfactual search: def make_feature(name, value): if type(value) is bool: return FeatureFactory.newBooleanFeature(name, value) else: return FeatureFactory.newNumericalFeature(name, value) ​ ​ features = [ make_feature(p[0], p[1]) for p in [ ("NewCreditCustomer", False), ("Amount", 2125.0), ("Interest", 20.97), ("LoanDuration", 60.0), ("Education", 4.0), ("NrOfDependants", 0.0), ("EmploymentDurationCurrentEmployer", 6.0), ("IncomeFromPrincipalEmployer", 0.0), ("IncomeFromPension", 301.0), ("IncomeFromFamilyAllowance", 0.0), ("IncomeFromSocialWelfare", 53.0), ("IncomeFromLeavePay", 0.0), ("IncomeFromChildSupport", 0.0), ("IncomeOther", 0.0), ("ExistingLiabilities", 8.0), ("RefinanceLiabilities", 6.0), ("DebtToIncome", 26.29), ("FreeCash", 10.92), ("CreditScoreEeMini", 1000.0), ("NoOfPreviousLoansBeforeLoan", 1.0), ("AmountOfPreviousLoansBeforeLoan", 500.0), ("PreviousRepaymentsBeforeLoan", 590.95), ("PreviousEarlyRepaymentsBefoleLoan", 0.0), ("PreviousEarlyRepaymentsCountBeforeLoan", 0.0), ("Council_house", False), ("Homeless", False), ("Joint_ownership", False), ("Joint_tenant", False), ("Living_with_parents", False), ("Mortgage", False), ("Other", False), ("Owner", False), ("Owner_with_encumbrance", True), ("Tenant", True), ("Entrepreneur", False), ("Fully", False), ("Partially", False), ("Retiree", True), ("Self_employed", False), ] ] We can confirm now, using the newly created PredictionProvider model directly, that this input will lead to a PaidLoan=false prediction: from trustyai.utils import toJList ​ model.predictAsync(toJList([PredictionInput(features)])).get()[0].getOutputs()[ 0 ].toString() 'Output{value=false, type=boolean, score=0.7835956811904907, name='PaidLoan'}' UNCONSTRAINED BASIC SEARCH To get started we will search for a counterfactual with no constraints at all. This is not a realistic use case, but we will use it as a baseline. n_features = len(features) ​ constraints = [False] * n_features We will also create a set of equal bounds for all the features. Again, this is not realistic, but, as mentioned, we do it to establish a baseline. Note that boolean features will ignore the bounds anyway (since they only have two possible values), so we can just create a set such as: features_boundaries = [NumericalFeatureDomain.create(0.0, 10000.0)] * n_features Next, we create a termination criteria for the search. We will use a 10 second time limit for the search and instantiate a new counterfactual explainer termination_config = TerminationConfig().withSecondsSpentLimit(Long.valueOf(10)) ​ solver_config = ( CounterfactualConfigurationFactory.builder() .withTerminationConfig(termination_config) .build() ) explainer = CounterfactualExplainer(solver_config) We want our goal to be the model predicting the loan will be paid (PaidLoad=true), so we specify it as: goal = [Output("PaidLoan", Type.BOOLEAN, Value(True), 0.0)] As before, we will wrap all this context in a CounterfactualPrediction object and search for a counterfactual. Then, we will confirm that our counterfactual changes the outcome, by predicting its outcome using the model: 'Output{value=true, type=boolean, score=0.6006738543510437, name='PaidLoan'}' And indeed it changes. We will now verify which features were changed: def show_changes(explanation, original): entities = explanation.getEntities() N = len(original) for i in range(N): name = original[i].getName() original_value = original[i].getValue() new_value = entities[i].asFeature().getValue() if original_value != new_value: print(f"Feature '{name}': {original_value} -&gt; {new_value}") ​ show_changes(explanation, features) Feature 'IncomeFromSocialWelfare': 53.0 -&gt; 53.31125429433703 Feature 'RefinanceLiabilities': 6.0 -&gt; 1.230474777192958 Feature 'PreviousEarlyRepaymentsCountBeforeLoan': 0.0 -&gt; 6.0 Feature 'Owner': false -&gt; true Feature 'Owner_with_encumbrance': true -&gt; false Here we can see the problem with the unconstrained search. Some of the fields that were changed (e.g. IncomeFromSocialWelfare, RefinanceLiabilities, etc) might be unfeasible to change in practice. This is where we should improve some of the initial counterfactual settings, namely the constraints and the search domain. CONSTRAINED SEARCH We will now try a more realistic search, which incorporates domain-specific knowledge (and common sense). To do so, we will constrain features we feel they shouldn’t (or mustn’t) change and specify sensible search bounds. We will start with the constraints: constraints = [ True, # NewCreditCustomer False, # Amount True, # Interest False, # LoanDuration True, # Education True, # NrOfDependants False, # EmploymentDurationCurrentEmployer False, # IncomeFromPrincipalEmployer False, # IncomeFromPension False, # IncomeFromFamilyAllowance False, # IncomeFromSocialWelfare False, # IncomeFromLeavePay False, # IncomeFromChildSupport False, # IncomeOther True, # ExistingLiabilities True, # RefinanceLiabilities False, # DebtToIncome False, # FreeCash False, # CreditScoreEeMini True, # NoOfPreviousLoansBeforeLoan True, # AmountOfPreviousLoansBeforeLoan True, # PreviousRepaymentsBeforeLoan True, # PreviousEarlyRepaymentsBefoleLoan True, # PreviousEarlyRepaymentsCountBeforeLoan False, # Council_house False, # Homeless False, # Joint_ownership False, # Joint_tenant False, # Living_with_parents False, # Mortgage False, # Other False, # Owner False, # Owner_with_encumbrance" False, # Tenant False, # Entrepreneur False, # Fully False, # Partially False, # Retiree False, # Self_employed ] The constraints should be self-explanatory, but in essence, they are divided into three groups. They can be attributes that: * you cannot or should not change (protected), for instance, age, education level, etc * you can change, for instance, loan duration, loan amount, etc * you probably won’t be able to change, but might be informative to change. For instance, you might not be able to easily change your income, but you might be interested in knowing how much it would need to be, in order to get the prediction as favourable. features_boundaries = [ None, # NewCreditCustomer NumericalFeatureDomain.create(0.0, 1000.0), # Amount None, # Interest NumericalFeatureDomain.create(0.0, 120.0), # LoanDuration None, # Education None, # NrOfDependants NumericalFeatureDomain.create(0.0, 40.0), # EmploymentDurationCurrentEmployer NumericalFeatureDomain.create(0.0, 1000.0), # IncomeFromPrincipalEmployer NumericalFeatureDomain.create(0.0, 1000.0), # IncomeFromPension NumericalFeatureDomain.create(0.0, 1000.0), # IncomeFromFamilyAllowance NumericalFeatureDomain.create(0.0, 1000.0), # IncomeFromSocialWelfare NumericalFeatureDomain.create(0.0, 1000.0), # IncomeFromLeavePay NumericalFeatureDomain.create(0.0, 1000.0), # IncomeFromChildSupport NumericalFeatureDomain.create(0.0, 1000.0), # IncomeOthe None, # ExistingLiabilities None, # RefinanceLiabilities NumericalFeatureDomain.create(0.0, 100.0), # DebtToIncome NumericalFeatureDomain.create(0.0, 100.0), # FreeCash NumericalFeatureDomain.create(0.0, 10000.0), # CreditScoreEeMini None, # NoOfPreviousLoansBeforeLoan None, # AmountOfPreviousLoansBeforeLoan None, # PreviousRepaymentsBeforeLoan None, # PreviousEarlyRepaymentsBefoleLoan None, # PreviousEarlyRepaymentsCountBeforeLoan None, # Council_house None, # Homeless None, # Joint_ownership None, # Joint_tenant None, # Living_with_parents None, # Mortgage None, # Other None, # Owner None, # Owner_with_encumbrance None, # Tenant None, # Entrepreneur None, # Fully None, # Partially None, # Retiree None, # Self_employed ] As before, we wrap this data in a CounterfactualPrediction, start a new search test that the counterfactual does change the outcome: 'Output{value=true, type=boolean, score=0.5038489103317261, name='PaidLoan'}' And we confirm that only unconstrained features were changed: show_changes(explanation, features) Feature 'LoanDuration': 60.0 -&gt; 56.947228037333545 Feature 'IncomeFromSocialWelfare': 53.0 -&gt; 59.6876474017064 Feature 'FreeCash': 10.92 -&gt; 10.914352713171315 MINIMUM COUNTERFACTUAL PROBABILITIES We can see that the previous counterfactual, although with the desired outcome, had an outcome probability close to 50%. It might be the case where we want a higher "confidence" in a counterfactual’s outcome. With TrustyAI we have the possibility to specify a minimum probability for the result (when the model supports prediction confidences). Let’s say we want a result that is at least 75% confident that the loan will be repaid. We can just encode the minimum probability as the last argument of each Output (the desired "confidence"). A minimum probability of 0 (as we’ve used) simply means that any desired outcome will be accepted, regardless of its probability. goal = [Output("PaidLoan", Type.BOOLEAN, Value(True), 0.75)] We can then re-run the search with all the data as defined previously and  check that the answer is what we are looking for, in terms of outcome: 'Output{value=true, type=boolean, score=0.7572674751281738, name='PaidLoan'}' And indeed, this time the counterfactual will lead to an outcome with confidence as we specified. And we show which features need to be changed for the desired outcome: show_changes(explanation, features) Feature 'LoanDuration': 60.0 -&gt; 14.899149688096976 Feature 'EmploymentDurationCurrentEmployer': 6.0 -&gt; 5.8223107382429395 Feature 'FreeCash': 10.92 -&gt; 10.942602612323316 Feature 'Joint_ownership': false -&gt; true This concludes the introduction on using TrustyAI’s explainability library from Python as well as some counterfactual basics. By using these Python bindings, we are able to use these features and easily integrate them with the available Python tools and ecosystem, such as interactive notebooks and plotting libraries, for instance. Happy coding! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/QTmRHgWcSTw" height="1" width="1" alt=""/&gt;</content><dc:creator>Rui Vieira</dc:creator><feedburner:origLink>http://feeds.athico.com/~r/droolsatom/~3/T0YquR_VYkE/using-trustyais-explainability-from-python.html</feedburner:origLink></entry><entry><title>Connect Node.js applications to Red Hat OpenShift Streams for Apache Kafka with Service Binding</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/iSaOJhIevik/connect-nodejs-applications-red-hat-openshift-streams-apache-kafka-service" /><author><name>Evan Shortiss</name></author><id>ac16015c-bb34-46b0-9433-f01e3db9c94e</id><updated>2021-07-27T07:00:00Z</updated><published>2021-07-27T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; is a vital piece of infrastructure for teams adopting an &lt;a href="https://developers.redhat.com/topics/event-driven"&gt;event-driven architecture&lt;/a&gt;. By connecting applications with minimal coupling, event-driven architecture lets teams create distributed, fault-tolerant applications using the runtimes most appropriate for the specific task and team. However, managing infrastructure and Kafka clusters is a complex and time-consuming task. A managed Kafka service such as &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; allows teams to focus on delivering applications, while Red Hat takes care of the Kafka infrastructure.&lt;/p&gt; &lt;p&gt;Once your Kafka infrastructure is in place, you'll want to start developing applications using your preferred runtimes. This article focuses on &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, which has become one of the &lt;a href="https://www.datadoghq.com/state-of-serverless/"&gt;most popular runtimes&lt;/a&gt; for cloud-native application development. Integrating Node.js applications with their organization's broader event-driven architecture based on Kafka is critical for developers.&lt;/p&gt; &lt;p&gt;This article demonstrates how to connect and authenticate your Node.js applications to OpenShift Streams for Apache Kafka using the &lt;a href="https://github.com/k8s-service-bindings/spec"&gt;Service Binding Specification for Kubernetes&lt;/a&gt;. The Service Binding spec says that it "aims to create a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;-wide specification for communicating service secrets to applications in an automated way." Figure 1 shows a high-level overview of the interactions between the components in this specification. Don’t worry if you’re not yet familiar with these components or concepts; this article walks you through each step.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/service-binding.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/service-binding.png?itok=o1TcKze1" width="600" height="289" alt="A Node.js application runs in a container on a managed Kafka instance. On an OpenShift cluster, Service Binding handles connection details and injects them into the container." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Architecture overview of Service Binding, injecting connection details into a Node.js application container on a managed Kafka instance. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: A &lt;a href="https://www.youtube.com/watch?v=RNtOIz2MZUw&amp;list=PLf3vm0UK6HKqZ3Vi7h1Ynfbi0TpdXUr25&amp;index=4"&gt;video demonstration&lt;/a&gt; of the process described in this article is available on the Red Hat Developer YouTube channel.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;You should be set up on the following services to carry out the techniques in this article:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/getting-started"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;. Click the &lt;strong&gt;Create a Kafka instance&lt;/strong&gt; button to get started. There is no charge for this OpenShift Streams for Apache Kafka instance.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. Use the &lt;strong&gt;Get started in the Sandbox&lt;/strong&gt; button to get access. This service is also free.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You will also need to download the following tools:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;a href="https://www.openshift.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt; command-line interface (CLI). Installation instructions are available &lt;a href="https://github.com/redhat-developer/app-services-guides/tree/main/rhoas-cli#installing-the-rhoas-cli"&gt;on GitHub&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The OpenShift CLI, available at &lt;a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/"&gt;an OpenShift repository&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Instructions to install the OpenShift CLI and use it to log into your cluster are available via the &lt;strong&gt;Command line tools&lt;/strong&gt; section of the OpenShift help menu. This screen is shown in Figure 2. Use the &lt;strong&gt;Copy Login Command&lt;/strong&gt; link to obtain a login command once you have the CLI installed.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screenshot%202021-07-21%20at%208.33.56%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Screenshot%202021-07-21%20at%208.33.56%20PM.png?itok=xTYtulKm" width="600" height="365" alt="You can download the oc command-line tool for many different operating systems from the Command Line Tools page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Download site for the oc command-line tool. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can follow the instructions in this article using your own OpenShift cluster instead of Developer Sandbox. If you choose to use your own cluster, you must install two additional tools: the &lt;a href="https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator"&gt;Service Binding Operator&lt;/a&gt;, which can be &lt;a href="https://github.com/redhat-developer/service-binding-operator"&gt;found on GitHub&lt;/a&gt;, and the &lt;a href="https://github.com/redhat-developer/app-services-operator"&gt;Red Hat OpenShift Application Services Operator&lt;/a&gt;. These operators are pre-installed in the sandbox.&lt;/p&gt; &lt;h2&gt;Provisioning a managed Kafka Instance on OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;This section will be brief because we’ve already &lt;a href="https://developers.redhat.com/articles/2021/06/28/getting-started-red-hat-openstreams-apache-kafka"&gt;written an article&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=RNtOIz2MZUw&amp;list=PLf3vm0UK6HKqZ3Vi7h1Ynfbi0TpdXUr25&amp;index=4"&gt;published a video&lt;/a&gt; covering the steps. You can create a managed Kafka instance using the &lt;a href="https://cloud.redhat.com/beta/application-services/streams/kafkas"&gt;OpenShift Streams for Apache Kafka UI&lt;/a&gt; or the following Red Hat OpenShift Application Services CLI commands:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Log in using a browser-based flow: &lt;pre&gt; &lt;code class="language-bash"&gt;$ rhoas login &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Create a managed Kafka instance named &lt;code&gt;nodejs-binding&lt;/code&gt;: &lt;pre&gt; &lt;code class="language-bash"&gt;$ rhoas kafka create nodejs-binding &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The Kafka cluster should finish provisioning after about three minutes. You can verify it’s ready using the &lt;code&gt;status&lt;/code&gt; field from the output of the &lt;code&gt;rhoas kafka list&lt;/code&gt; command or from the &lt;strong&gt;status&lt;/strong&gt; column in the user interface (UI).&lt;/p&gt; &lt;p&gt;Once the Kafka cluster is provisioned, you can create the topic that will be used in this article as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Enter the following command and follow the interactive prompts to choose a Kafka instance or context: &lt;pre&gt; &lt;code class="language-bash"&gt;$ rhoas kafka use &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Create an orders topic: &lt;pre&gt; &lt;code class="language-bash"&gt;$ rhoas kafka topic create orders --partitions 3 &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Now you should have a Kafka instance with an &lt;code&gt;orders&lt;/code&gt; topic running in the cloud. You’re ready to deploy a Node.js application and connect it to your Kafka instance.&lt;/p&gt; &lt;h2&gt;Deploying a Node.js Kafka producer on OpenShift&lt;/h2&gt; &lt;p&gt;The source code for the application used in this article is available in the &lt;a href="https://github.com/evanshortiss/rhosak-nodejs-sbo-example"&gt;OpenShift Streams Node.js Service Binding Example&lt;/a&gt; repository on GitHub. The code is a typical Node.js application that uses the &lt;a href="https://expressjs.com/"&gt;Express&lt;/a&gt; web framework and a &lt;a href="https://kafka.js.org/"&gt;KafkaJS&lt;/a&gt; client to interact with a Kafka cluster. Users submit a web form exposed by this application, and the form's inputs generate a record in the &lt;code&gt;orders&lt;/code&gt; Kafka topic.&lt;/p&gt; &lt;p&gt;What’s interesting about this application is that it uses the &lt;a href="https://github.com/nodeshift/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; module to retrieve a Kafka configuration that’s managed by the Red Hat OpenShift Application Services Operator and Service Binding Operator. The &lt;code&gt;kube-service-bindings&lt;/code&gt; module reads configuration data that has been mounted into the application container. The configuration is compatible with the Service Binding specification.&lt;/p&gt; &lt;h3&gt;Create a Kafka producer&lt;/h3&gt; &lt;p&gt;Reading the mounted configuration and creating a Kafka producer requires just a few lines of Node.js code that run when the application starts:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const { Kafka } = require('kafkajs') const { getBinding } = require('kube-service-bindings') module.exports = async getKafkaProducer (log) =&gt; { // Obtain the a kafkajs format configuration to connect to a // Red Hat OpenShift Streams for Apache Kafka cluster const cfg = getBinding('KAFKA', 'kafkajs') // Create the Kafka instance and a producer const kafka = new Kafka(cfg) const producer = kafka.producer() // Return a connected producer object await producer.connect() return producer }&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Deploy the Node.js application&lt;/h3&gt; &lt;p&gt;Use the OpenShift CLI to deploy the application. Run the following commands to deploy the Node.js application and expose an HTTP endpoint to access it:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Define a variable to hold the name of the OpenShift project where the Node.js application will be deployed: &lt;pre&gt; &lt;code class="language-bash"&gt;$ export PROJECT=&lt;your-project-name&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can list available projects using the &lt;code&gt;oc projects&lt;/code&gt; command.&lt;/p&gt; &lt;/li&gt; &lt;li&gt;Select the project. It will be used as a context for subsequent commands: &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc project $PROJECT &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Define a variable to hold the URL of the image on quay.io that you will deploy: &lt;pre&gt; &lt;code class="language-bash"&gt;$ export IMAGE=quay.io/evanshortiss/rhosak-nodejs-sbo-example &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Deploy the image as a container and expose an endpoint: &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app $IMAGE -l app.openshift.io/runtime=nodejs $ oc expose svc rhosak-nodejs-sbo-example&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The application will attempt to start, but it’s coded to verify that the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; environment variable is set. Because we haven't set this variable yet, the application knows that it cannot retrieve a Kafka connection configuration and fails to start. Figure 3 shows this behavior and the Node.js pod log output. We'll fix the problem in the next section.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/node_sb_fail.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/node_sb_fail.jpg?itok=111XSJF6" width="600" height="366" alt="Logs from the Node.js application report when the SERVICE_BINDING_ROOT environment variable is missing." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Indication that the Node.js application cannot start due to a missing configuration. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Creating a KafkaConnection and Service Binding&lt;/h2&gt; &lt;p&gt;Addressing the Node.js application’s missing configuration is straightforward, thanks to the Red Hat OpenShift Application Services CLI.&lt;/p&gt; &lt;h3&gt;Create a KafkaConnection custom resource&lt;/h3&gt; &lt;p&gt;Enter the following command and follow the prompts to create a &lt;code&gt;KafkaConnection&lt;/code&gt; custom resource. The name you use for &lt;code&gt;$PROJECT&lt;/code&gt; must be the same project into which you deployed the Node.js application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ rhoas cluster connect --namespace $PROJECT&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you receive an error about exceeding the maximum number of service accounts, use the &lt;code&gt;rhoas serviceaccount delete&lt;/code&gt; command to remove an inactive service account.&lt;/p&gt; &lt;p&gt;Once the &lt;code&gt;rhoas cluster connect&lt;/code&gt; command has run, a &lt;code&gt;KafkaConnection&lt;/code&gt; custom resource is created in your OpenShift project, along with two secrets. The first secret is named &lt;code&gt;rh-cloud-services-accesstoken-cli&lt;/code&gt; and is used by the Red Hat OpenShift Application Services Operator to communicate with cloud.redhat.com APIs. The other secret is named &lt;code&gt;rh-cloud-services-service-account&lt;/code&gt;. It contains the Simple Authentication and Security Layer (SASL) username and password your application requires to authenticate with your managed Kafka instance. Use the OpenShift CLI to describe the &lt;code&gt;KafkaConnection&lt;/code&gt; resource, as shown in Figure 4. Note that it refers to these secrets.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kafkaconnection.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kafkaconnection.jpg?itok=IOcYOyN9" width="600" height="366" alt="KafkaConnection custom resource information is being rendered in a terminal using the OpenShift CLI." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: KafkaConnection custom resource and its content. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Create the Service Binding&lt;/h3&gt; &lt;p&gt;The last step is to bind this information to your Node.js application. Enter the following command and follow the prompts to create a Service Binding:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ rhoas cluster bind --namespace $PROJECT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command creates a &lt;code&gt;ServiceBinding&lt;/code&gt; customer resource. The Service Binding Operator uses this resource to update the Node.js application &lt;code&gt;Deployment&lt;/code&gt; with the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; environment variable and mount a volume containing the Kafka connection configuration. You can run the &lt;code&gt;oc describe deployment/rhosak-nodejs-sbo-example&lt;/code&gt; command to confirm that the deployment has been updated with this configuration.&lt;/p&gt; &lt;h2&gt;Verifying the Node.js producer and Kafka connectivity&lt;/h2&gt; &lt;p&gt;At this point, the Node.js application is running. Use the &lt;strong&gt;Open URL&lt;/strong&gt; button from the OpenShift UI's Topology view, as shown in Figure 5, to load the application homepage in your browser.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screenshot%202021-06-29%20at%2011.50.19%20AM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Screenshot%202021-06-29%20at%2011.50.19%20AM.png?itok=OYEQ74uB" width="600" height="366" alt="The Node.js application is up and running, thanks to Service Binding." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Node.js application showing a healthy state in the OpenShift UI, thanks to Service Binding. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Place a few orders for ice cream using the application UI. Each order you submit is produced to the &lt;code&gt;orders&lt;/code&gt; topic of the managed Kafka cluster running on Red Hat OpenShift Streams for Apache Kafka. The &lt;code&gt;Receipt Email&lt;/code&gt; field shown in Figure 6 is used as a key when producing the record.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screenshot%202021-06-29%20at%2011.55.58%20AM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Screenshot%202021-06-29%20at%2011.55.58%20AM.png?itok=OCemoIga" width="600" height="366" alt="The Node.js application UI provides a Receipt Email field whose value is used as the Kafka message key." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: The Node.js application's UI. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;You can verify the orders are written to Kafka using a tool such as &lt;a href="https://github.com/edenhill/kafkacat"&gt;Kafkacat&lt;/a&gt;. The Kafkacat command to consume the orders in your terminal is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# These variables can be obtained from the OpenShift Streams UI or using the # rhoas kafka describe and rhoas serviceaccount CLI commands $ export KAFKA_BOOTSTRAP_SERVERS=&lt;replace-me&gt; % export KAFKA_CLIENT_ID=&lt;replace-me&gt; $ export KAFKA_CLIENT_SECRET=&lt;replace-me&gt; $ kafkacat -t orders -b $KAFKA_BOOTSTRAP_SERVERS \ -X sasl.mechanisms=PLAIN \ -X security.protocol=SASL_SSL \ -X sasl.username=$KAFKA_CLIENT_ID \ -X sasl.password=$KAFKA_CLIENT_SECRET -K " / " -C &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 7 shows the Kafkacat output. The email address is the key, and the JSON is the message value. Orders from the same email address will be processed in a series because they are routed to the same partition.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kafkacat.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/kafkacat.jpg?itok=WpMqWVDF" width="600" height="366" alt="Kafkacat output shows the user's email address as the key and the order information in JSON as the message value." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: Kafkacat consuming orders produced by the Node.js application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;In this article, you’ve learned how to connect your Node.js applications to Red Hat OpenShift Streams for Apache Kafka using Service Binding. If you’re wondering how this supports local development environments against a managed Kafka instance, take a look at the &lt;a href="https://github.com/evanshortiss/rhosak-nodejs-sbo-example#develop-locally"&gt;local development section&lt;/a&gt; of the sample application repository—it’s refreshingly straightforward.&lt;/p&gt; &lt;p&gt;Using managed services such as OpenShift Dedicated (which underlies the Developer Sandbox) and OpenShift Streams for Apache Kafka allows you to focus on building applications instead of infrastructure.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Interested in learning more?&lt;/strong&gt; Why not try creating a consumer application that processes the entries from the &lt;code&gt;orders&lt;/code&gt; topic that were produced using this article. You can use this &lt;a href="https://github.com/redhat-developer/app-services-guides/tree/main/code-examples/quarkus-kafka-quickstart"&gt;Quarkus Kafka application&lt;/a&gt; as a template.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/27/connect-nodejs-applications-red-hat-openshift-streams-apache-kafka-service" title="Connect Node.js applications to Red Hat OpenShift Streams for Apache Kafka with Service Binding"&gt;Connect Node.js applications to Red Hat OpenShift Streams for Apache Kafka with Service Binding&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/iSaOJhIevik" height="1" width="1" alt=""/&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2021-07-27T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/27/connect-nodejs-applications-red-hat-openshift-streams-apache-kafka-service</feedburner:origLink></entry></feed>
